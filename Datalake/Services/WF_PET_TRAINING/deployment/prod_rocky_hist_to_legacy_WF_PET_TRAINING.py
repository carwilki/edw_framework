# Databricks notebook source
from pyspark.sql.functions import col
import csv

tables = ['TRAINING_CATEGORY_TYPE','TRAINING_CATEGORY_TYPE_FOCUS_AREA','TRAINING_CLASS_PROMO','TRAINING_CLASS_TYPE','TRAINING_CLASS_TYPE_DETAIL', 'TRAINING_FOCUS_AREA','TRAINING_INVOICE', 'TRAINING_PACKAGE', 'TRAINING_PACKAGE_OPTION', 'TRAINING_PACKAGE_OPTION_CLASS_TYPE', 'TRAINING_PACKAGE_OPTION_DETAIL', 'TRAINING_PACKAGE_PROMO','TRAINING_PET','TRAINING_PET_FOCUS_AREA','TRAINING_RESERVATION', 'TRAINING_RESERVATION_HISTORY','TRAINING_SCHED_CHANGE_CAPTURE', 'TRAINING_SCHED_CHANGE_STATE','TRAINING_SCHED_CHANGE_TYPE', 'TRAINING_SCHED_CLASS_TYPE', 'TRAINING_SCHED_ENTITY_TYPE', 'TRAINING_SCHED_STORE_CLASS', 'TRAINING_SCHED_STORE_CLASS_DETAIL', 'TRAINING_SCHED_TRAINER', 'TRAINING_STORE_BLACKOUTS', 'TRAINING_STORE_CLASS', 'TRAINING_STORE_CLASS_DETAIL', 'TRAINING_UNIT_OF_MEASURE', 'TRAINING_CUSTOMER','PET_TRAINING_RESERVATION']

for table in tables:    
  print(table)
  rocky_table = f"refine.{table}_history"
  target_table = f"legacy.{table}"
  df = spark.sql(f"select * from {rocky_table}")
  df = df.drop(col("bd_create_dt_tm"),col("bd_update_dt_tm"),col("source_file_name"))
  df.write.insertInto(f"{target_table}",overwrite=True)
